#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Cruzamento de Dados - SEFAZ-MS
AnÃ¡lise comparativa entre Carta de ServiÃ§o e Site SEFAZ

Este script realiza:
1. Carregamento e unificaÃ§Ã£o dos datasets
2. IdentificaÃ§Ã£o de serviÃ§os similares
3. PadronizaÃ§Ã£o de categorias e perfis
4. ValidaÃ§Ã£o de URLs
5. GeraÃ§Ã£o de relatÃ³rio executivo
"""

import pandas as pd
import requests
import re
import time
from collections import Counter, defaultdict
from difflib import SequenceMatcher
from urllib.parse import urlparse
import csv
from datetime import datetime

class CruzamentoDados:
    def __init__(self):
        self.carta_servico_path = '../carta-de-servico/sefaz_servicos.csv'
        self.site_sefaz_path = '../site-sefaz/sefaz_site_servicos.csv'
        self.output_path = 'base_dados_unificada.csv'
        
        # Dados carregados
        self.df_carta = None
        self.df_site = None
        self.df_unificado = None
        
        # AnÃ¡lises
        self.servicos_similares = []
        self.categorias_mapeadas = {}
        self.perfis_mapeados = {}
        self.urls_validadas = {}
        
        # EstatÃ­sticas
        self.stats = {
            'total_servicos': 0,
            'servicos_duplicados': 0,
            'urls_validas': 0,
            'urls_invalidas': 0,
            'categorias_unificadas': 0,
            'perfis_unificados': 0
        }
    
    def carregar_dados(self):
        """Carrega os datasets dos dois projetos"""
        print("ðŸ“Š Carregando datasets...")
        
        try:
            # Carta de ServiÃ§o
            self.df_carta = pd.read_csv(self.carta_servico_path)
            self.df_carta['fonte'] = 'Carta de ServiÃ§o'
            print(f"   âœ… Carta de ServiÃ§o: {len(self.df_carta)} serviÃ§os")
            
            # Site SEFAZ
            self.df_site = pd.read_csv(self.site_sefaz_path)
            self.df_site['fonte'] = 'Site SEFAZ'
            print(f"   âœ… Site SEFAZ: {len(self.df_site)} serviÃ§os")
            
            # Padronizar colunas
            self._padronizar_colunas()
            
            print(f"   ðŸ“ˆ Total: {len(self.df_carta) + len(self.df_site)} serviÃ§os carregados")
            
        except Exception as e:
            print(f"   âŒ Erro ao carregar dados: {e}")
            raise
    
    def _padronizar_colunas(self):
        """Padroniza nomes das colunas entre os datasets"""
        colunas_padrao = ['Categorias', 'Perfis', 'ServiÃ§os', 'URL', 'fonte']
        
        # Verificar se as colunas existem
        for df, nome in [(self.df_carta, 'Carta'), (self.df_site, 'Site')]:
            colunas_faltantes = [col for col in colunas_padrao[:-1] if col not in df.columns]
            if colunas_faltantes:
                print(f"   âš ï¸  {nome}: Colunas faltantes: {colunas_faltantes}")
    
    def identificar_servicos_similares(self, threshold=0.7):
        """Identifica serviÃ§os similares entre os dois datasets"""
        print(f"\nðŸ” Identificando serviÃ§os similares (threshold: {threshold})...")
        
        servicos_carta = self.df_carta['ServiÃ§os'].tolist()
        servicos_site = self.df_site['ServiÃ§os'].tolist()
        
        similares_encontrados = 0
        
        for i, servico_carta in enumerate(servicos_carta):
            for j, servico_site in enumerate(servicos_site):
                similaridade = SequenceMatcher(None, 
                                             servico_carta.lower(), 
                                             servico_site.lower()).ratio()
                
                if similaridade >= threshold:
                    self.servicos_similares.append({
                        'servico_carta': servico_carta,
                        'servico_site': servico_site,
                        'similaridade': round(similaridade, 3),
                        'categoria_carta': self.df_carta.iloc[i]['Categorias'],
                        'categoria_site': self.df_site.iloc[j]['Categorias'],
                        'perfil_carta': self.df_carta.iloc[i]['Perfis'],
                        'perfil_site': self.df_site.iloc[j]['Perfis'],
                        'url_carta': self.df_carta.iloc[i]['URL'],
                        'url_site': self.df_site.iloc[j]['URL']
                    })
                    similares_encontrados += 1
        
        print(f"   âœ… {similares_encontrados} pares de serviÃ§os similares encontrados")
        self.stats['servicos_duplicados'] = similares_encontrados
    
    def mapear_categorias(self):
        """Mapeia e padroniza categorias entre os datasets"""
        print("\nðŸ“‚ Mapeando e padronizando categorias...")
        
        # Extrair todas as categorias
        categorias_carta = set()
        categorias_site = set()
        
        for categorias in self.df_carta['Categorias'].dropna():
            if ';' in str(categorias):
                categorias_carta.update([cat.strip() for cat in str(categorias).split(';')])
            else:
                categorias_carta.add(str(categorias).strip())
        
        for categoria in self.df_site['Categorias'].dropna():
            categorias_site.add(str(categoria).strip())
        
        print(f"   ðŸ“Š Carta de ServiÃ§o: {len(categorias_carta)} categorias Ãºnicas")
        print(f"   ðŸ“Š Site SEFAZ: {len(categorias_site)} categorias Ãºnicas")
        
        # Mapear categorias similares
        self._mapear_categorias_similares(categorias_carta, categorias_site)
        
        print(f"   âœ… {len(self.categorias_mapeadas)} mapeamentos de categoria criados")
    
    def _mapear_categorias_similares(self, categorias_carta, categorias_site, threshold=0.6):
        """Mapeia categorias similares entre os datasets"""
        for cat_carta in categorias_carta:
            melhor_match = None
            melhor_score = 0
            
            for cat_site in categorias_site:
                score = SequenceMatcher(None, cat_carta.lower(), cat_site.lower()).ratio()
                if score > threshold and score > melhor_score:
                    melhor_score = score
                    melhor_match = cat_site
            
            if melhor_match:
                self.categorias_mapeadas[cat_carta] = {
                    'categoria_site': melhor_match,
                    'similaridade': round(melhor_score, 3),
                    'categoria_unificada': self._unificar_categoria(cat_carta, melhor_match)
                }
    
    def _unificar_categoria(self, cat1, cat2):
        """Cria nome unificado para categoria"""
        # LÃ³gica para criar nome unificado (pode ser refinada)
        if len(cat1) <= len(cat2):
            return cat1
        return cat2
    
    def mapear_perfis(self):
        """Mapeia e padroniza perfis entre os datasets"""
        print("\nðŸ‘¥ Mapeando e padronizando perfis...")
        
        perfis_carta = set(self.df_carta['Perfis'].dropna())
        perfis_site = set(self.df_site['Perfis'].dropna())
        
        print(f"   ðŸ“Š Carta de ServiÃ§o: {len(perfis_carta)} perfis Ãºnicos")
        print(f"   ðŸ“Š Site SEFAZ: {len(perfis_site)} perfis Ãºnicos")
        
        # Mapeamento manual baseado no conhecimento dos dados
        mapeamento_perfis = {
            'CidadÃ£o / Ã“rgÃ£o Governamental': 'CidadÃ£o',
            'ComÃ©rcio, IndÃºstria e ServiÃ§os': 'Empresa',
            'AgropecuÃ¡ria': 'Produtor Rural',
            'FiscalizaÃ§Ã£o': 'Poder PÃºblico'
        }
        
        self.perfis_mapeados = mapeamento_perfis
        print(f"   âœ… {len(self.perfis_mapeados)} mapeamentos de perfil criados")
    
    def validar_urls(self, sample_size=50):
        """Valida uma amostra de URLs para verificar se estÃ£o funcionais"""
        print(f"\nðŸ”— Validando URLs (amostra de {sample_size})...")
        
        # Combinar todas as URLs
        todas_urls = list(self.df_carta['URL'].dropna()) + list(self.df_site['URL'].dropna())
        
        # Selecionar amostra
        import random
        urls_amostra = random.sample(todas_urls, min(sample_size, len(todas_urls)))
        
        validas = 0
        invalidas = 0
        
        for url in urls_amostra:
            try:
                response = requests.head(url, timeout=10, allow_redirects=True)
                status = response.status_code
                
                if status == 200:
                    self.urls_validadas[url] = {'status': 'vÃ¡lida', 'codigo': status}
                    validas += 1
                elif status == 404:
                    self.urls_validadas[url] = {'status': 'nÃ£o encontrada', 'codigo': status}
                    invalidas += 1
                else:
                    self.urls_validadas[url] = {'status': 'outro', 'codigo': status}
                    invalidas += 1
                    
            except Exception as e:
                self.urls_validadas[url] = {'status': 'erro', 'erro': str(e)}
                invalidas += 1
            
            # Rate limiting
            time.sleep(0.5)
        
        self.stats['urls_validas'] = validas
        self.stats['urls_invalidas'] = invalidas
        
        print(f"   âœ… URLs vÃ¡lidas: {validas}")
        print(f"   âŒ URLs invÃ¡lidas: {invalidas}")
        print(f"   ðŸ“Š Taxa de sucesso: {(validas/(validas+invalidas)*100):.1f}%")
    
    def criar_base_unificada(self):
        """Cria base de dados unificada"""
        print("\nðŸ”„ Criando base de dados unificada...")
        
        # Aplicar mapeamentos
        df_carta_mapped = self.df_carta.copy()
        df_site_mapped = self.df_site.copy()
        
        # Mapear perfis na carta de serviÃ§o
        df_carta_mapped['Perfis'] = df_carta_mapped['Perfis'].map(
            lambda x: self.perfis_mapeados.get(x, x)
        )
        
        # Combinar datasets
        self.df_unificado = pd.concat([df_carta_mapped, df_site_mapped], ignore_index=True)
        
        # Adicionar colunas de anÃ¡lise
        self.df_unificado['id_unico'] = range(len(self.df_unificado))
        self.df_unificado['data_extracao'] = datetime.now().strftime('%Y-%m-%d')
        
        # Salvar
        self.df_unificado.to_csv(self.output_path, index=False, encoding='utf-8')
        
        self.stats['total_servicos'] = len(self.df_unificado)
        print(f"   âœ… Base unificada criada: {len(self.df_unificado)} registros")
        print(f"   ðŸ’¾ Salva em: {self.output_path}")
    
    def gerar_relatorio_executivo(self):
        """Gera relatÃ³rio executivo para a chefia"""
        print("\nðŸ“‹ Gerando relatÃ³rio executivo...")
        
        relatorio = f"""
# RelatÃ³rio Executivo - Cruzamento de Dados SEFAZ-MS

**Data**: {datetime.now().strftime('%d/%m/%Y %H:%M')}
**ResponsÃ¡vel**: AnÃ¡lise Automatizada

## Resumo Executivo

Este relatÃ³rio apresenta os resultados do cruzamento de dados entre o CatÃ¡logo de ServiÃ§os e o Site Principal da SEFAZ-MS, visando identificar oportunidades de melhoria e padronizaÃ§Ã£o.

## MÃ©tricas Principais

### Volume de Dados
- **Total de ServiÃ§os**: {self.stats['total_servicos']}
- **Carta de ServiÃ§o**: {len(self.df_carta)} serviÃ§os
- **Site SEFAZ**: {len(self.df_site)} serviÃ§os

### AnÃ¡lise de DuplicaÃ§Ã£o
- **ServiÃ§os Similares Identificados**: {self.stats['servicos_duplicados']}
- **Taxa de SobreposiÃ§Ã£o**: {(self.stats['servicos_duplicados']/min(len(self.df_carta), len(self.df_site))*100):.1f}%

### ValidaÃ§Ã£o de URLs
- **URLs VÃ¡lidas**: {self.stats['urls_validas']}
- **URLs InvÃ¡lidas**: {self.stats['urls_invalidas']}
- **Taxa de Funcionalidade**: {(self.stats['urls_validas']/(self.stats['urls_validas']+self.stats['urls_invalidas'])*100):.1f}%

## AnÃ¡lise de Categorias

### DistribuiÃ§Ã£o por Fonte
"""
        
        # AnÃ¡lise de categorias
        categorias_carta = self.df_carta['Categorias'].value_counts().head(5)
        categorias_site = self.df_site['Categorias'].value_counts().head(5)
        
        relatorio += "\n#### Top 5 Categorias - Carta de ServiÃ§o\n"
        for cat, count in categorias_carta.items():
            relatorio += f"- **{cat}**: {count} serviÃ§os\n"
        
        relatorio += "\n#### Top 5 Categorias - Site SEFAZ\n"
        for cat, count in categorias_site.items():
            relatorio += f"- **{cat}**: {count} serviÃ§os\n"
        
        # AnÃ¡lise de perfis
        relatorio += "\n## AnÃ¡lise de Perfis\n\n"
        
        perfis_carta = self.df_carta['Perfis'].value_counts()
        perfis_site = self.df_site['Perfis'].value_counts()
        
        relatorio += "### DistribuiÃ§Ã£o por Perfil\n\n"
        relatorio += "| Perfil | Carta de ServiÃ§o | Site SEFAZ |\n"
        relatorio += "|--------|------------------|------------|\n"
        
        todos_perfis = set(perfis_carta.index) | set(perfis_site.index)
        for perfil in sorted(todos_perfis):
            count_carta = perfis_carta.get(perfil, 0)
            count_site = perfis_site.get(perfil, 0)
            relatorio += f"| {perfil} | {count_carta} | {count_site} |\n"
        
        # RecomendaÃ§Ãµes
        relatorio += f"""

## Principais Achados

### ðŸŽ¯ Oportunidades de Melhoria

1. **PadronizaÃ§Ã£o de Categorias**
   - {len(self.categorias_mapeadas)} categorias similares identificadas
   - Oportunidade de unificaÃ§Ã£o de nomenclaturas

2. **EliminaÃ§Ã£o de DuplicaÃ§Ãµes**
   - {self.stats['servicos_duplicados']} serviÃ§os potencialmente duplicados
   - Necessidade de revisÃ£o e consolidaÃ§Ã£o

3. **ManutenÃ§Ã£o de URLs**
   - {self.stats['urls_invalidas']} URLs com problemas identificadas
   - Requer atualizaÃ§Ã£o ou correÃ§Ã£o

### ðŸ“Š Insights EstratÃ©gicos

1. **Cobertura de ServiÃ§os**
   - Boa distribuiÃ§Ã£o entre perfis de usuÃ¡rio
   - Foco empresarial em ambas as plataformas

2. **Qualidade dos Dados**
   - Taxa de funcionalidade de URLs: {(self.stats['urls_validas']/(self.stats['urls_validas']+self.stats['urls_invalidas'])*100):.1f}%
   - Base de dados robusta e confiÃ¡vel

## RecomendaÃ§Ãµes

### Curto Prazo (1-2 meses)
- [ ] Corrigir URLs invÃ¡lidas identificadas
- [ ] Revisar serviÃ§os duplicados
- [ ] Padronizar nomenclatura de categorias principais

### MÃ©dio Prazo (3-6 meses)
- [ ] Implementar base de dados unificada
- [ ] Criar processo de sincronizaÃ§Ã£o entre portais
- [ ] Desenvolver dashboard de monitoramento

### Longo Prazo (6+ meses)
- [ ] IntegraÃ§Ã£o completa dos portais
- [ ] Sistema de gestÃ£o unificado de serviÃ§os
- [ ] AutomaÃ§Ã£o de validaÃ§Ã£o de URLs

## ConclusÃ£o

O cruzamento de dados revelou oportunidades significativas de melhoria na gestÃ£o dos serviÃ§os da SEFAZ-MS. A implementaÃ§Ã£o das recomendaÃ§Ãµes propostas resultarÃ¡ em:

- **Melhor experiÃªncia do usuÃ¡rio** atravÃ©s da eliminaÃ§Ã£o de duplicaÃ§Ãµes
- **Maior eficiÃªncia operacional** com processos padronizados
- **ReduÃ§Ã£o de custos** de manutenÃ§Ã£o atravÃ©s da unificaÃ§Ã£o

---

**PrÃ³ximos Passos**: ApresentaÃ§Ã£o dos resultados e definiÃ§Ã£o de cronograma de implementaÃ§Ã£o.
"""
        
        # Salvar relatÃ³rio
        with open('relatorio_executivo_cruzamento.md', 'w', encoding='utf-8') as f:
            f.write(relatorio)
        
        print("   âœ… RelatÃ³rio executivo gerado: relatorio_executivo_cruzamento.md")
    
    def salvar_analises_detalhadas(self):
        """Salva anÃ¡lises detalhadas em arquivos separados"""
        print("\nðŸ’¾ Salvando anÃ¡lises detalhadas...")
        
        # ServiÃ§os similares
        if self.servicos_similares:
            df_similares = pd.DataFrame(self.servicos_similares)
            df_similares.to_csv('servicos_similares.csv', index=False, encoding='utf-8')
            print("   âœ… ServiÃ§os similares: servicos_similares.csv")
        
        # Mapeamento de categorias
        if self.categorias_mapeadas:
            with open('mapeamento_categorias.csv', 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['Categoria_Carta', 'Categoria_Site', 'Similaridade', 'Categoria_Unificada'])
                for cat_carta, dados in self.categorias_mapeadas.items():
                    writer.writerow([
                        cat_carta,
                        dados['categoria_site'],
                        dados['similaridade'],
                        dados['categoria_unificada']
                    ])
            print("   âœ… Mapeamento de categorias: mapeamento_categorias.csv")
        
        # URLs validadas
        if self.urls_validadas:
            with open('validacao_urls.csv', 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['URL', 'Status', 'Codigo_HTTP', 'Erro'])
                for url, dados in self.urls_validadas.items():
                    writer.writerow([
                        url,
                        dados['status'],
                        dados.get('codigo', ''),
                        dados.get('erro', '')
                    ])
            print("   âœ… ValidaÃ§Ã£o de URLs: validacao_urls.csv")
    
    def executar_analise_completa(self):
        """Executa anÃ¡lise completa de cruzamento de dados"""
        print("ðŸš€ Iniciando AnÃ¡lise de Cruzamento de Dados SEFAZ-MS")
        print("=" * 60)
        
        try:
            # Carregar dados
            self.carregar_dados()
            
            # AnÃ¡lises
            self.identificar_servicos_similares()
            self.mapear_categorias()
            self.mapear_perfis()
            self.validar_urls()
            
            # Gerar outputs
            self.criar_base_unificada()
            self.gerar_relatorio_executivo()
            self.salvar_analises_detalhadas()
            
            print("\n" + "=" * 60)
            print("âœ… AnÃ¡lise de cruzamento concluÃ­da com sucesso!")
            print(f"ðŸ“Š {self.stats['total_servicos']} serviÃ§os processados")
            print(f"ðŸ” {self.stats['servicos_duplicados']} duplicaÃ§Ãµes identificadas")
            print(f"ðŸ”— {self.stats['urls_validas']}/{self.stats['urls_validas']+self.stats['urls_invalidas']} URLs vÃ¡lidas")
            
        except Exception as e:
            print(f"\nâŒ Erro durante a anÃ¡lise: {e}")
            raise

def main():
    """FunÃ§Ã£o principal"""
    cruzamento = CruzamentoDados()
    cruzamento.executar_analise_completa()

if __name__ == "__main__":
    main()